{
  "title": "Unit 1 Â - Introduction to Parallel Computing",
  "cells": [
    {
      "type": "markdown",
      "data": "## Section 1 - Introduciton\n#### Motivation\n\n***Parallel Computing:*** Is the computation performed on a *parallel computer* that:\n+ Executes concurrently several instructions for the resolution of a single problem\n\n***Sequential Computer:*** Follows the conventional (von Neuman) architecture and instructions are executed sequentially one after the other"
    },
    {
      "type": "markdown",
      "data": "#### Why parallel Computing\n\nThere is a tendency to include more parallelism support in the hardware:\n+ Clock frequency limited by the light speed\n+ Larger integration scale unfeasible due to heating problems\n\nAs well as, sequential computing does not cover the needs:\n+ Large scale problems\n+ Real time"
    },
    {
      "type": "markdown",
      "data": "**Simulation:** emulate a physical system using a computer, they often require a large computation capacity. (Virtual prototyping, complex geometry...)\n**Data processing of large scientific challenges:** require acquiring, storing and processing large data volumes. (High energy physics => Particle colliders)"
    },
    {
      "type": "markdown",
      "data": "#### Supercomputing\nAlso known as parallel computing or High preformance computing => Consists on the concurrent execution of different tasks by different processing units. For example:\n+ *Single processor:*\n    + Several cores in the same processor sharing some key units\n    + Simultaneous execution of multiple basic instructions\n+ *Single Computer:*\n    + Integration of different processors in a single computer\n    + Depending on the coupling of processors:\n        + Shared memory\n        + Distributed memory\n+ *Local Network*\n+ *Internet*"
    },
    {
      "type": "markdown",
      "data": "#### Shared Memory\n+ All processors can access to the whole memory\n+ Different latencies depending on the memory bank accessed (due to proximity)\n+ Scalability up to hundreds of processors\n+ High cost, high preformance in fine-grain parallelism\n\n#### Distributed Memory\n+ Each processor has only acces to its local memory block\n+ Information is explicitly exchange through messages\n+ Higher scalability (thousands of processors)\n+ Reduced cost, worse performance in the fine-grain"
    },
    {
      "type": "markdown",
      "data": "## Section 2 - Parallel Computing Models\n\n#### Flynn Taxonomy\n+ **SISD** Single Instruction, Single Data (Sequential Computer)\n+ **SIMD** Single Instruction, Multiple Data (Vector Computers, processors with multimedia extension)\n+ **MISD** Multiple Instruction, Single Data \n+ **MIMD** Multiple Instruction, Multiple Data (Multiprocessors, multi-core)"
    },
    {
      "type": "markdown",
      "data": "#### Memory Architectures\n+ Shared Memory - Single address space for all the processors\n    + Adv: Easy Programming\n    + Dis: Scalability, price\n+ Distributed Memory - requires a communication network to let processors access to data outside of the local space\n    + There is no global memory of concept\n    + Independent processors\n    + Data exchange explicitly programmed\n        + Adv: Scalability, cost\n        + Dis: Programming effort\n+ *Hybrid Architectures* - Distributed-Shared Memory\n    + Each node is a multiprocessor\n    + Communiaction to move data from one node to another\n    + `Super computers follow often this model`\n\n+ Multi-Core Processors - Current tendency in the design of processors\n    +  Symmetric (or not) multiprocessing in a single chip\n    +  Several cache levels in the same chip\n        + Adv: Cost\n        + Dis: Low efficiency (badnwidth)\n+ Many-Core Processors - massively parallel, with a large number of single cores\n    + GPU\n    + Many Cores\n    + Light threads, very quick context change\n        + Adv: Cost, power\n        + Dis: Complex Programming"
    },
    {
      "type": "markdown",
      "data": "#### Interconnection Networks\n**Static topologies:** ring, open 2D grid...\n**Dynamic networks:** single-stage, multi-stage, crossbar\n+ Uniform latency networks: low scalability (cost)\n+ Non-uniform latency networks: cheaper and latency depending on the distance "
    },
    {
      "type": "markdown",
      "data": "#### Clusters\nIs simple a set of PCs or workstations connected in a network to execute parallel computing algorithms. Currently:\n+ Rack Structure\n+ A set of nodes: 2 multi-cores, 1 disk (optional GPU)\n+ Network infraestructure\n    + ethernet and low-latency network (infiniband, myrinet)\n+ Node *front-end*"
    },
    {
      "type": "markdown",
      "data": "#### Shared memory model\n**PRAM** - Parallel Random Access Machine\n+ All processes can access to any memory position (constant time)\n+ All processes execute the same code (eventually some parts may depend on the process index)\n+ Information exchanged through variables"
    },
    {
      "type": "markdown",
      "data": "#### Distributed memory model\n*p* Processors, each one with its won address space (local memory)\n+ All processes share the same code (with parts depending on the process index)\n+ Information is exchanged explicitly using messages\n+ Local processing and communication instructions (sends/recieve through \"interconnection network\")"
    },
    {
      "type": "markdown",
      "data": "#### Parallel Programming Methodologies\n+ Automatic Parallelisation \n+ Semi-Automatic Parallelisation\n+ New Programming Languages\n+ Extensions of conventional Languages\n+ Software Libraries (API)"
    },
    {
      "type": "markdown",
      "data": "#### Single/Multiple Program Multiple Data\nHigh-level programming models, suitable for any of the previous models\n**SPMD:** The same program is executed in all the tasks\n> SPMD programs are easy to mantain, although they may require conditional instructions\n\n**MPMD:** Each task may have a different program"
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    }
  ]
}