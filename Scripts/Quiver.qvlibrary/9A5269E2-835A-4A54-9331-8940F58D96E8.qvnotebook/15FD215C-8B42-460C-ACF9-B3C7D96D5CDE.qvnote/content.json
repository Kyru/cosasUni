{
  "title": "Unit 2 - Shared Memory. Basic Parallel Algorithms Design",
  "cells": [
    {
      "type": "markdown",
      "data": "## Section 1 - Shared Memory Model\n#### Concurrent processes\nThey are typically defined using *fork-join-like* constructions\n+ ***Fork*** creates a new concurrent task that starts its execution in the same point where the initiator task made the fork\n+ ***Join*** waits for the task to finish\n\nThis schema can be implemented at the level of:\n+ OS processes (*heavy processes*)\n+ Threads (*light processes*)\n\n#### Shared Memory Model\n+ Tasks share a common memory space\n+ Programming quite similar to sequential case\n    + There are no task \"private\" data\n    + We do not need to exchange data explicitly\n+ Inconvenients\n+ Memory access need to be coordinated\n    + Locks, monitors...\n    + Unpredictable results if data access is not properly protected\n+ Data locality is difficult to control\n\n#### Thread model\nThis model is coupled to shared memory architectures\n***thread:*** Independent instruction flow that can be scheduled by the OS\n+ A process may have multiple concurrent execution threads\n+ Each thread has local data\n+ Threads share resources/memory of the process\n+ Synchronization is needed\n\n#### Java Threads"
    },
    {
      "type": "code",
      "language": "java",
      "data": "// Object-oriented model\n\npublic class HelloThread extends Thread {\n  public void run() {\n    System.out.print(\"Hello from a thread!\");\n  }\n  public static void main(String args[]) {\n    (new HelloThread()).start();\n  }\n}\n\n// Synchronized methods\n\npublic class SynchronizedCounter {\n  private int c = 0;\n  publid synchronized void increment() {\n    c++;\n  } \n  public synchronized int value() {\n    return c;\n  }\n}"
    },
    {
      "type": "markdown",
      "data": "#### POSIX threads (pthreads)\nSome operations:\n+ Creations: pthread\\_create, pthread_join\n+ Locking: sem\\_wait, sem_post\n+ Mutual exclusion: mutex\\_lock, mutex_unlock\n+ Condition variables: pthread\\_cond_ wait, pthread\\_cond\\_signal, pthread\\_cond_broadcast\n\nDrawbacks:\n+ Portability (windows uses another system)\n+ Task-oriented parallelism, rather than data-oriented parallelism"
    },
    {
      "type": "markdown",
      "data": "#### OpenMP\nStandarization of threads\n+ Based on compiler directives\n+ Available in C/C++ and Fortran\n+ Portable/multi-plataform\n+ Easy to use: incremental parallelisation\n+ Sequential code can be preserved\n\n*Some directives and functions:*\n+ `pragma omp parallel for`\n+ `omp_get_thread_num()`\n\n> Creation and termination of threads is implicit in some directives (no need of calling fork/join)"
    },
    {
      "type": "markdown",
      "data": "#### Unix processes\nEach process comprises information about resources and its execution status\n+ Program code (read-only, can be shared)\n+ Variables (global, heap and stack)\n+ Execution context: registres, stack pointer, etc...\n+ System resources (only accessible through the OS)\n    + Identifiers \n    + Environment, work directory, signals\n    + File descriptors\n+ In multi-threaded processes\n    + Each thread has its own execution context\n    + Each thread has its own independent stack \n    + System resources are shared"
    },
    {
      "type": "markdown",
      "data": "#### Threaded-memory model\nSimple model: Single address memory space\nMore realistic model: unique space of addresses with private variables for each thread\n\n![Captura de pantalla 2017-10-02 a las 15.36.34.png](quiver-image-url/59469D0F4608CB485F95D40AAED3C3B8.png =401x173)\n\nEach thread has its own stack\n+ Some variables are created in the stack (local variables)\n+ A thread cannot know if a stack from other thread is active"
    },
    {
      "type": "markdown",
      "data": "#### Memory Access Coordination\n\nThe exchange of information among threads is performed by reading and writing on variables in the shared memory space\n> Simultaneous access can produce a race condition (final result could be incorrect)"
    },
    {
      "type": "markdown",
      "data": "#### Mutual Exclusion and Synchronization\nHow race conditions can be addressed?\n*Atomic operations*\n+ Force problematic operations to be performed atomically (without being interrupted)\n+ Special instructions of the processor: test-and-set or compare-and-set\n\n*Critical sections*\n+ Code fragments with more than one instruction\n+ Only one thread can execute the section simultaneously\n+ It requires *synchronization* mechanisms: semaphores\n+ Risk of dead-locks\n\n*Event-based synchronization*\n+ Barriers: all threads wait in the barrier for the last one to arrive\n+ Ordered execution and others"
    },
    {
      "type": "markdown",
      "data": "## Section 2 - Fundamentals of Parallel Algorithm Design\n#### Parallelization of Algorithms\nParalellizing an algorithm implies finding *concurrent tasks* (parts of the algorithm that can be run in parallel)\n> A task can only start when another one has finished"
    },
    {
      "type": "markdown",
      "data": "#### Data dependencies\nIt is possible to determine if there exist dependencies between two tasks form the I/O data of each task\n\n![Captura de pantalla 2017-09-26 a las 12.13.16.png](quiver-image-url/3CF9A6B2C96971B2283EAC4294665F76.png =448x141)\nDependency types:\n+ Flow dependencies (condition 1 not fulfilled)\n+ Anti-dependecy (condition 2 is not fulfilled)\n+ Output dependency (condition 3 is not fulfilled)"
    },
    {
      "type": "code",
      "language": "c_cpp",
      "data": "// Flow dependency\ndouble a = 3, b = 5, c, d;\nc = T1(a,b);\nd = T2(a, b, c);\n\n/* T2 cannot start until T1 ends */\n\n// Anti-dependency\ndouble a[10],b[10],c[10],y;\nT1(a,b,&y);\nT2(b,c,a);\n\n/* T2 cannot start until T1 ends, otherwise T2 would overwrite the contents of a that is input to T1 */\n\n// Output dependecy\ndouble a[10],b[10],c[10],x[5];\nT1(a,b,x);\nT2(c,b,x);\n\n/* Both tasks modify array x */"
    },
    {
      "type": "markdown",
      "data": "#### Design of Parallel Algorithms: General Idea\n\nBasically 2 phases:\n1. Task decompostion\n    + Requires a detailed analysis of the problem => Task Dependecy Graph\n2. Task assignment\n    + Which thread/process executes each task\n    + Often implies agglomeration of several tasks\n     \nUsually there are several possible parallelization strategies\n+ Use one decomposition or another may have a great impact on performance\n+ We must try to maximize the ***degree of concurrency***"
    },
    {
      "type": "markdown",
      "data": "#### Task Dependecy Graphs\n\nTDGs are an abstraction used to express the dependencies among the tasks and their relative execution order.\n+ It can be represented by using a Directed Acyclic Graph (DAG)\n+ Nodes denotes the tasks (may have an associated cost)\n+ Edges define the dependencies among tasks\n\n*Definitions*\n+ Length of a path: sum of the costs *c<sub>i</sub>* of each node contained in the path\n+ Critical path: longest path between the starting and final nodes\n+ Maximum *concurrency degree*: Larger number of tasks that can be concurrently executed\n+ Average concurrency degree: $M = \\sum_{N}^{i = 1} C_{i}/L  $\n    + N = total nodes, L = length of the critical path\n\n![Captura de pantalla 2017-10-02 a las 16.21.57.png](quiver-image-url/15419085E5E416E81D5766B532F541FE.png =462x348)\n\n![Captura de pantalla 2017-10-02 a las 16.28.43.png](quiver-image-url/7611309406D82D9D5B423A1AEB7F684A.png =461x354)\n\n> Sometimes the graph incorporates information related to *communication*"
    },
    {
      "type": "markdown",
      "data": "## Section 3 - Performance Evaluation\n#### Performance Evaluation\nThe main objective of parallel computing is to increase performance.\n+ It is very important to know how the different parts of a parallel program behave\n+ It is also imporatant to know how they will behave when the number of processors and the size of the program change"
    },
    {
      "type": "markdown",
      "data": "#### Analysis types\n*A priori* Analysis:\n+ It is performed on the pseudocode and the program desgin, before the implementation of a program\n+ Independent of the machine where it is executed\n+ It enables identifying the best approach to implement a parallel program\n+ It enable the determination of the best size of the problem and the features of the hardware used\n\n*A posteriori* Analysis:\n+ It is performed on a specific implementation and machine, and using a defined of input data\n+ It enables analysing bottlenecks and detecting unpredicted conditions during the design"
    },
    {
      "type": "markdown",
      "data": "#### Theoretical Analysis\nThe cost is analyzed depending on the problem size: n\nIn many cases the cost depends only on *n: t(n)*\nHowever, sometimes given the same problem size, different behaviour may be observed depending on the input data\n+ Cost of the most favourable case\n+ Cost of the less favourable case\n+ Average case (reasonable when the cost only depends on the problem size)\n\n> In practical terms, asymptotic values are defined (inferior and superior)"
    },
    {
      "type": "markdown",
      "data": "#### Concept of a FLOP\nFLOP - Floating Point Operation => measurement unit for:\n+ Cost of algorithms\n+ Performance of computers (flop/s)\n\n1 flop = cost of an elemental floating point operation (product, sum, division, subtraction)\n+ The cost of the elemental integer operations are neglectable\n+ The cost of other operations in floating point are evaluated depending on the FLOP unit\n\n> The flop represents a machine-independent cost measurement unit"
    },
    {
      "type": "markdown",
      "data": "#### Asymptotic Notation\n\n+ O Notation:\n    + It defines an upper bound, except for constants and asintotically the shape of the function growin\n    + In practical terms it is the highest order term of the cost expression without considering its coefficient\n    \n+ o notation:\n    + It also considers the coefficient of the highest-order term\n    + Useful when comparing two algorithms which has the same order O"
    },
    {
      "type": "markdown",
      "data": "#### Parameters to evaluate the performance\n\n**Absolute** parameters:\n+ They enable knowing the real cost of parallel algorithms\n+ They are the basis for the computation of relative parameters that are used to compare algorithms\n+ They are the most important ones for real-time problems\n\n**Relative** parameters:\n+ They enable comparing parallel algorithms among them and with respect to the sequential implementations\n+ They provide information about the rate of usage of processors"
    },
    {
      "type": "markdown",
      "data": "#### Absolute Parameters\n+ Execution time of a sequential algorithm: t(n)\n+ Execution time of a parallel  algorithm: t(n,p)\n    + Arithmetic time: ta(n,p)\n    + Communication time: tc(n,p)\n+ Total cost: C(n,p)\n+ *Overhead:* to(n,p)\n\n*Notation*\n+ When the problem size is always n, we can avoid the n => t(p)\n+ Subindices instead of functions => Tp, Cp"
    },
    {
      "type": "markdown",
      "data": "#### Execution time \nTime spent in the execution by the sequential algorithm (using only one processor, t(n)) or by the parallel algorithm (in p processors, t(n,p))\n+ It only takes into account the number of floating point operations\n+ A priori cost is measured in FLOPs\n+ Experimentally the cost will be measured in seconds\n\n![Captura de pantalla 2017-10-22 a las 11.20.46.png](quiver-image-url/1BC45570AF1309851EA3BFE9D3310C5C.png =641x474)"
    },
    {
      "type": "markdown",
      "data": "#### Total cost and Overhead\n\nThe execution of a parallel algorithm normally implies an extra time with respect to the sequential algorithm\nThe parallel ***total cost*** accounts for the total time empoyed by a parallel algorithm.\n`C(n,p) = p.t(n,p)`\nThe ***overhead*** indicates which is the added cost with respect to the sequential algorithm\n`to(n,p) = C(n,p) - t(n)`"
    },
    {
      "type": "markdown",
      "data": "#### Speed-up and Efficiency\nThe **Speed-up** denotes the speed gaining of a parallel algorithm with respect its sequential version\n`S(n,p) = t(n)/t(n,p)`\nThe reference time t(n) could be: \n+ The best sequential algorithm at our knowledge\n+ The parallel algorithm using 1 processor\n\nThe **efficiency** measures the degree of usage of the parallel units by an algorithm\n`E(n,p) = S(n,p)/p`\nIt is normally expressed as a percentage (either in the frame 0-100% or 0-1)\n\n![Captura de pantalla 2017-10-22 a las 11.53.27.png](quiver-image-url/644778416B497CE82294633376EA0F9E.png =610x410)\n\nIn this case, p=2 and p=3 would give the same speedup, but not the same efficiency. In p=2 processes will be group in pairs. In p=3 in two groups, where T4 which is the leftout would be asigned to the first finishing thread. In p=3 the efficiency will be much lower because threads will have to wait until T1, T2, T3 and T4 end."
    },
    {
      "type": "markdown",
      "data": "#### How to obtain good performance\n\nIdeally we look for p processors with a speedup of p (efficiency is 1). To achieve this:\n+ Appropriate **parallelization design:**\n    + Well balance load distribution\n    + Minimize time in which processors are idle\n    + Minimum possible overhead\n+ Specify aspects of the **architecture where it runs**\n    + Different in shared memory of message passing\n    + **Data access time** is not considered in the theoretical cost analysis, but it is very important in current architectures\n\n![Captura de pantalla 2017-10-22 a las 12.10.22.png](quiver-image-url/0BACF1875520099A02BBF8ACE4874FBE.png =607x485)"
    },
    {
      "type": "markdown",
      "data": "## Section 4 - Algorithm Design: Task Decomposition\n#### Parallel Algorithms Design\n\nParallel algorithms have a higher design complexity than sequential ones\n+ Concurrency (communication and synchronixation)\n+ Data and code allocation to processors\n+ Concurrent access to shared data\n+ Scalability for an increasing number of processors"
    },
    {
      "type": "markdown",
      "data": "#### Task decomposition\n> **Task** each one of the computation units defined by the programmer which can be potentially be executed in parallel\n\nThe process of spliting computations in task is called **decomposition**\n\nGranularity:\n+ The decomposition can be *fine-grained* or *coarse-grained*\n+ Usually a fine grain decomposition is performed by then concurrent operations are grouped into coarser tasks"
    },
    {
      "type": "markdown",
      "data": "#### Domain Decomposition techniques\n+ Data are split in chunks of similar size (sub-domains)\n+ A task is assigned to each domain, which will perform the needed operations on the sub-domain's data\n+ Typically used when all sub-domains data require the same set of operations\n+ These techniques are classified in:\n    + Output data centred decompositions\n    + Input data centred decompositions\n    + Block-oriented decompositions (matrix algorithms)\n\n![Captura de pantalla 2017-10-22 a las 13.07.40.png](quiver-image-url/8B7C66D79B7F981DF0A57BC29EFC96F7.png =621x463)\n\n![Captura de pantalla 2017-10-22 a las 13.09.31.png](quiver-image-url/3794A1A1197DCD6B930B65BA5BE35960.png =639x455)"
    },
    {
      "type": "markdown",
      "data": "#### Functional decomposition directed by the data flow\n+ It is used when a problem can be split into phases\n+ Each phase executes a different algorithm\n+ Typically, it involves the next steps:\n    1. Different phases are identified\n    2. A task is assigned to each phase\n    3. Data requirements for each task is analysed"
    },
    {
      "type": "markdown",
      "data": "## Section 5 - Algorithmic Schemes (I)\n#### Algorithmic Schemes\n\nThey are used parallelization approaches:\n+ A schema is used to solve a wide range of problems\n+ A problem may require several schemes\n\nSome schemes:\n+ Data parallelism/partitioning\n+ Task parallelsim (master-slave, process farm, replicated workers)\n+ Tree and graph based schemes (divide and conquer)\n+ Synchronous Parallelism\n+ *(pipelinning)*"
    },
    {
      "type": "markdown",
      "data": "#### Divide and Conquer\n\nA method to obtain concurrency in problems that can be solved using the *divide and conquer* technique\n1. Divide the original problem in two or more subproblems\n2. In turn these subproblems are divided in two or more subproblems, and so until the base case is reached\n3. Obtained data are appropriately combined to obtain the final result\n\nSeveral types of tasks:\n+ **Dividing** the problem: it is performed in the inner nodes to create child nodes\n+ **Solving** the base case: only in the leaves of the tree\n+ **Combining** the result: performed in the inner nodes, they collapse the associated sub-tree\n\nExample:\n+ *Quicksort* -> splitting stage has the largest cost\n+ *Merge-sort* -> focuses work on combination"
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    },
    {
      "type": "markdown",
      "data": ""
    }
  ]
}